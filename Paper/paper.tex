% Based on the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.

\begin{document}
%
\title{Subtle Gestures with the J!NS MEME}
%
\author{Thomas Bartel \and
Florian Bossert \and
Sajjad Ahmad}
%
% TODO should we use 'et al.' here? ~FB
\authorrunning{T. Bartel \and F. Bossert \and S. Ahmad}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Karlsruhe Institute of Technology, Karlsruhe, Germany}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in
15--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%
\section{Introduction}
\subsection{A Subsection Sample}
Please note that the first paragraph of a section or subsection is
not indented. The first paragraph that follows a table, figure,
equation etc. does not need an indent, either.

Subsequent paragraphs, however, are indented.

% As a reference, maybe useful later
Current smartglasses:
Smartglasses with cameras: Spectacles by Snap Inc. https://www.spectacles.com/
Smartglasses with EOG sensors: J!ns Meme by J!ns (website currently only in Japanese):
https://jins-meme.com/

% Assigned to FB
\section{Related Work}
Our work touches on multiple areas of research, including wearable sensing, gesture
recognition, and subtle interaction. In the following section we roughly outline
previous works in these areas.

\subsection{The Human Body as an Input Interface}
There are various works discussing the use of the human body as an input interface, with
most of them having an exploratory nature:
Harrison et al. resolved the location of finger tapping on the user's arm and hand
using acoustic sensing \cite{10.1145/1753326.1753394}.
Serrano et al. explored Hand-to-Face input by tracking infrared markers applied to
fingers with an optical system \cite{10.1145/2556288.2556984}.
Lisserman et al. proposed the human ear as an input surface, which can be used by
touching it in certain ways. This was achieved by placing electrodes around the ear
\cite{10.1145/2468356.2468592}.
Even eyelid gestures have been looked into by Jota et al., who developed a framework
and prototypes for them \cite{10.5555/2788890.2788938}.
Using a battery-free low-power wrist band, Truong et al. managed to classify hand
gestures by detecting skin deformations \cite{10.1145/3274783.3274854}.

Especially when using extremities as input methods, inertial measurement units (IMUs),
which consist of accelerometers and gyroscopes, have been widely employed: Wen et al.
used them to recognize unremarkable fine-motor finger movements
\cite{10.1145/2858036.2858466}. Laput et al. utilized the IMUs in commercially available
smartwatches with a custom performance-boosting kernel to classify a wide variety of hand
gestures \cite{10.1145/2984511.2984582}.

As for applications of wearable sensing, there are many from convenient user input
to medical monitoring:
Ike et al. demonstrated using hand gestures to control a TV's content navigation interface
with a IMU-equipped wrist band \cite{10.1145/2641248.2641359}.
Ohnishi et al. used sensors in shoes to track exercises and general posture
\cite{10.1145/3174910.3174938}.
Schiboni et al. monitored drinking with wrist-worn inertial sensors, demonstrating
the feasibility of long-term sparse natural gesture recognition
\cite{10.1145/3267242.3267253}.
On the more medical side, Milosevic et al. performed jump performance analysis with
low-cost wearable devices instead of pressure-sensitive force plates, with the intent
of making training progress monitoring more easily available \cite{10.1145/2753509.2753512}.
Again for automatic monitoring, Bedri et al. used wearable sensors to track
eating episodes instead of making patients rely on often biased self-written food intake
journals \cite{10.1145/3130902}.

\subsection{EOG Sensing and Subtle Interaction}
While the aforementioned work utilized mainly IMUs, touch sensors or optical systems,
Electrooculography (EOG) has become a useful tool for eye-tracking in recent years, too. 
In 2006, Manabe et al. attached EOG sensors to over-ear \cite{10.1145/1125451.1125655}
and in 2013, to in-ear head phones \cite{10.1145/2493988.2494329} to detect eye gestures.
Bulling et al. first demonstrated efficient real-time eye-movement recognition
with EOG goggles in 2009, which they custom-made \cite{10.1145/1520340.1520468}.
Once commercial EOG glasses became available in 2014, Ichimaru et al. used them for
activity recognition \cite{10.1145/2638728.2638795}. They used the J!ns Meme
smartglasses to distinguish between typing, reading, eating and talking.
As for more recent work, Li et al. used the J!ns Meme to infer the physical and social
context of the wearer by recognizing various greeting gestures involving kissing
\cite{10.1145/3384657.3384801}.

In contrast to the work discussed, which often employed noticeable or obviously unusual
hardware, we explicitly focus on subtle interaction. Pohl et al. define
four types of subtle interaction: (1) signifying feedback that is non-intrusive to the
user, (2) hiding interaction from others and potentially deceiving them, (3) employing
less effort for input and generally doing less, and (4) nudging users
\cite{10.1145/3290605.3300648}. Our work aims at type (2) social subtlety, like e.g.
Ashbrook et al. did with their tooth-click gesture recognition interface
\cite{10.1145/2935334.2935389}. With a similar goal, Li et al. developed TongueBoard,
a retainer form-factor device for recognizing silent speech \cite{10.1145/3311823.3311831}.

Itchy Nose by Lee et al. is the most closely related work to ours,
as it inspired our efforts \cite{10.1145/3123021.3123060}. They used the J!ns Meme
smartglasses to detect multiple gestures that consisted of rubbing or pushing their
noses, which can also be detected by the glasses' EOG sensors.

% Assigned to FB
\section{Analysis}
While there is quite a bit of research about the J!ns Meme and its applications, there
seemed to be room for more gestures to be scientifically explored, especially for the
purpose of subtle interaction. Therefore we set out to experiment with various
gestures to find out how well the J!ns Meme captured them. Our method of detection and
classification was an artificial neural network (ANN). ANNs have proven themselves as
a reliable way of classifying data, be it static data like images or text or sequential
data like audio recordings or sensor readouts.

\subsection{Workflow}
\begin{figure}
\centering
\centerline{\includegraphics[scale=0.8]{workflow.pdf}}
\caption{The workflow of this project}
\label{fig:workflow}
\end{figure}

We began with planning the rough outline that our research would follow.
The plan was as seen in Figure \ref{fig:workflow}:
\begin{itemize}
    \item Gather gesture ideas
    \item Check ideas for detectable responses
    \item Create tools to automate the recording of gestures
    \item Collect labelled data with the tools
    \item Model, train and validate a neural network until its performance is sufficient
    \item Test the network on our withheld test dataset
\end{itemize}

\subsection{Gestures}
% TODO, maybe use self-made illustrations here - do we even need them?
In the first phase, we came up with gestures we considered subtle. Early candidates were:
\begin{itemize}
    \item Pushing your glasses up your nose [see GIF]
    \item A thinking pose, where you touch your chin with your thumb and your cheek with your
        index finger [see GIF]
    \item Slow nodding [see GIF]
    \item Tapping the frame of the glasses [see GIF]
\end{itemize}

Since some of the gestures generated barely any responses on the EOG or the IMU readouts
[see figures with an example of a strong and a weak response],
we continued coming up with gestures until we had a total of seven (eleven, if left and
right variants are differentiated) presumably recognizable gestures.
The gestures we came up with and their apparent responses can be seen in table
\ref{tab:responses}.

\begin{table}
\centering
\caption{The quality of the EOG and IMU sensor responses for our gestures. Gestures 
        with (L/R) at the end have both a left and a right variant.}\label{tab:responses}
\begin{tabular}{|l|l|}
    \hline
    Gesture description & Response quality\\
    \hline
    Pushing glasses up the nose & strong\\
    Readjusting glasses (L/R) & strong\\
    Thinking pose & weak\\
    Slow nodding & strong\\
    Tilting head sideways (L/R) & strong\\
    Stroking mustache & weak\\
    Biting lip & weak\\
    Tapping on the nose (L/R)& strong\\
    Rubbing the nose & strong\\
    Pushing cheek up with a finger (L/R) & strong\\
    Tapping the glasses (L/R) & strong\\\hline
\end{tabular}
\end{table}

\subsection{Tools for Recording and Labelling}
To use the J!ns Meme, we first had to interface with it. The interface would have to allow
both recording and streaming the data from the smartglasses, perferrably.
The J!ns Meme's sensor data can be read either via an iOS, Android or macOS app
or via a Windows app, with the latter requiring a custom bluetooth dongle. For
initial gesture evaluation we used the Android app, but since we set out to have
live classification with the potential to control your computer, and only Windows
machines were available, we chose the Windows app J!ns Meme DataLogger
\cite{jinsMemeDataLoggerWin}. A software development kit is available for Android, macOS
and Windows as well \cite{jinsMemeSDK}, but it did not eliminate the
need for the bluetooth dongle on Windows and only implied a bigger overhead for us,
since the DataLogger can both record and forward the J!ns Meme's sensor data to a
file and a TCP port, respectively.

%TODO check correctness, this is from memory alone
The J!ns Meme DataLogger supplies eleven values per cycle, three of which are the
accelerometers' data and the remaining eight being at least partially processed EOG data.
Notably, we observed that we had different values available via the Android app
\cite{jinsMemeDataLoggerAndroid}. Namely there were gyroscope values, albeit they
were constantly zero, and less EOG values per time step.
We found this behavior to be the same on all J!ns Meme smartglasses and Android phones
we had available, but are not certain if it is intended behavior.

Since we wanted our neural network to have the most possible data, we simply included all
eleven values that the DataLogger on Windows provided into our data collection.

The next step after the data collection would have been the labelling of the recorded data.
But with our goal being at least hundreds of samples, manual labelling seemed impractical.
Therefore, we developed a tool that would firstly present our test subjects the
gestures they would have to perform and secondly ask them to press a button to
automatically document the start and end of their gesture. This allowed us to split the
file that the DataLogger would create into multiple files, taking the included time
stamps as a reference.

\subsection{Neural Network Architecture and Training} \label{subsec:nnArchAndTraining}
For the classification task of telling the gestures apart from each other, we decided on
a long short-term memory (LSTM) neural network. LSTM neural networks, contrary to convolutional neural networks
or multilayer perceptrons, allow for the input data to have different lengths. This
implies that we do not need to add padding to our data samples to bring them all to the
same length. Also, the data we use as input for the network is sequential and a time
series. Recurrent neural networks such as LSTMs can remember what data they were supplied
in the last time steps by having backward edges in their hidden layers, making them
usually a good fit for sequential data.
%TODO: elaborate why the other models from our baseline are probably less suited than LSTMs

% Assigned to TB
\section{Implementation}

The following sections give a detailed description of our implementation process. In total we implemented four programs during our research concerning the J!NS MEME glasses. In the first step we implemented a data collection tool that supported our test subjects during the collection and labeling process. As already described in our analysis section, we then decided to implement a neural network based on an LSTM architecture. After that we created a third program that uses several other state of the art machine learning techniques to predict the correct gestures by using the data from the J!NS MEME. This was a necessary step to show that our new LSTM-based architecture is able to perform just as well or even outperform other already established machine learning techniques in this gesture prediction task. The last step of our implementation process was a program that uses real time data from the J!NS MEME glasses to predict the gestures that a user performs.

\subsection{Data collection tool}

The prediction of gesture classes by using the J!ns meme data is a supervised classification task. This means that a large amount of labeled data is needed to train our LSTM-based neural network and the other machine learning techniques. One of our goals of this project was the creation of a neural network that achieves good results in the gesture prediction task regardless of which person is currently using the J!NS MEME glasses. To achieve this goal, it was necessary for us to select a diverse group of people that each collect a part of our training data. In total we selected ? (noch einfügen) people for the collection task.
\par
In order to create useful data for the machine learning techniques, you first have to collect the gesture data and afterwards assign them the correct label. We unified this process by creating a data collection tool that automatically assigns the correct label to each gesture after they are performed by the user.
\begin{figure}
\centering
\centerline{\includegraphics[scale=0.5]{Language_Selection.png}}
\caption{The language selection page of our data collection tool}
\label{fig:gesturestart}
\end{figure}
Figure \ref{fig:gesturestart} shows how the data collection tool lets the user choose a preferred language which is either german or english before the data collection starts. This language selection process is followed by a description of the experiment, which informs the user about what kind of data is collected and how the user is supposed to collect this data. These initial steps supply the user with all the important information that is needed to ensure a clean data collection process.
\par
After the initial steps the data collection process can start. We use a JSON file to configure which gestures the tool collects data for. 
\begin{figure}[ht]
\centering
\centerline{\includegraphics[width=\textwidth]{GestureJSON.png}}
\caption{A gesture defined in the JSON file of the data collection tool}
\label{fig:gesturejson}
\end{figure}
Figure \ref{fig:gesturejson} shows an entry for the gesture PUSH\_GLASSES\_UP in the JSON file. For every gesture we define a name and description that is available in either english or german. The "sampleCount" specifies the amount of data samples our tool will collect for the defined gesture. We decided that 10 samples for each gesture is a good compromise between the time that is needed to finish the collection process and the amount of data that we receive for our task. The "label" is just the label that our tool assigns to the data samples of this gesture. This label is later used for our supervised classification task. The "file" entry is the name of an .mp4 file that shows an example of how this gesture is supposed to be performed. This type of configuration allows us to freely define any kind of gesture that the test subjects need to perform.
\begin{figure}
\centering
\centerline{\includegraphics[scale=0.3]{Gesture_Description.png}}
\caption{The tool asks the test subject to collect data for a gesture}
\label{fig:gesturedatacollection}
\end{figure}
Figure \ref{fig:gesturedatacollection} shows how our tool asks a the user to collect data for a gesture. It shows a description of the gesture and how many samples of this gesture still need to be collected. As soon as the user is ready to perform this gesture, he can press the start button. In the next step the user needs to perform the described gesture. After he is finished the user needs to press the stop button. 
\begin{figure}
\centering
\centerline{\includegraphics[width=\textwidth]{Label_Assignment.png}}
\caption{The triples of start time, end time and label that the tool creates}
\label{fig:labelTriple}
\end{figure}
\par
Figure \ref{fig:labelTriple} shows the data that our tool saves when a gesture is performed. In the background our data collection tool saves the time when the start and when the stop button were pressed. This time interval is combined with the label that needs to be assigned to the data that was collected during this time. The triples of [start time, end time, label] allow us to automatically label the data that is collected from the J!NS MEME during the collection process. The procedure gets repeated until every gesture that is defined in the JSON file has reached its defined sample count.
After a test subject finished the collection process, we extract the file with the data that was collected from the J!ns meme glasses. We created a script that uses the triples of [label, start time, end time] of our data collection tool to separate the collected data into files that only contain the data of a single gesture. After this step we are left with a fully labeled gesture dataset that can be used to train the different machine learning architectures.

\subsection{The LSTM-based neural network}

As already described in the analysis section of this paper we decided to use an LSTM-based architecture for our neural network because every gesture that is performed while wearing the J!ns meme glasses creates a stream of data. LSTM's use recurrent connections in their hidden states which allows them to remember information from previous timesteps which is ideal for our use case.
\par
For our implementation we made use of the pytorch library \cite{NEURIPS2019_9015}. The library consists of many already implemented features that are needed for the deep learning process. We especially made use of the pytorch LSTM implementation. To train our neural network we shuffled our automatically labeled gesture dataset and split it into three parts. We used 60\% of the data as our training dataset. Another 20\% was then used as our validation dataset that we used to tune our hyperparameters. The last 20\% was then used as a test dataset to see how good our network performs on unseen data.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{LSTM_Architecture.png}
\caption{The architecture of our LSTM-based neural network}
\label{fig:lstmarchitecture}
\end{figure}
Figure \ref{fig:lstmarchitecture} shows the complete architecture of our LSTM-based neural network.
\paragraph{Input layer}
Our input layers consists of the raw data stream of the J!ns meme glasses. During the training of the network the input has a shape of (64, x, 11). The first dimension is the batch size that we used during our training. We tested several different numbers and concluded that 64 was a good compromise between training time and performance. The second dimension is the sequence length of our input. Every gesture that is performed by a person is always unique which also means that the time, that is needed to perform a gesture is always different. That also means that the sequence length of a gesture is always a variable number. This is no problem in our use case because we decided to use an LSTM as the base of our neural network and they are especially made for sequences with variable length. The third dimension of the input is the feature count. In every timestep the J!NS MEME produces a data vector that contains 11 features.
\paragraph{LSTM layer}
The input is then fed into an LSTM layer. This LSTM consists of 512 hidden states. We also tried several different hidden sizes for the LSTM and 512 was a good compromise between performance and training time. Small hidden sizes like 64 proved to be too small to completely learn our training data set while large hidden sizes like 1024 took too long to train. In our LSTM layer we used the built in dropout function of the pytorch library in order to prevent the overfitting of our network. We used the standard dropout rate of 0.5.
\paragraph{Layer normalization}
The output of the LSTM layer is then put into a Layer normalization layer. This layer normalizes the LSTM output across the feature dimensions. This allows us to use higher learning rates, makes the network less dependent on weight initialization and introduces some noise to prevent overfitting.
\paragraph{Linear layers}
In the last step of our network we need to classify the output of our LSTM. For this task we use several stacked linear layers. Each linear layer gets progressively smaller until we reach the last layer that maps from a size of 64 to a size of 11 which is the amount of gestures that we trained our network for. Each layer except for the last one uses the Exponential Linear Unit acitvation function. The last layer uses the Softmax activation function which outputs pseudo probabilities for each of our gestures. During the test time the gesture that has the maximum probability gets assigned to the input. Between each linear layer we also used a dropout rate of 25\% to prevent our network from overfitting.

\subsection{Other machine learning techniques}

To be able to compare the performance of our LSTM-based neural network we implemented several other state of the art machine learning techniques and trained them with our dataset. To implement those machine learning techniques we made use of the scikit-learn python library \cite{scikit-learn}. In total we implemented a Random Forest Classifier, a Decision Tree, a Naive Bayes Classifier, a Support Vector Machine and a K-Nearest Neighbor Classifier. The scikit-learn library already gave us access to standard implementations of those classifiers. These implementations all use a certain "fit()" function that automatically trains the classifier on a given dataset. One challenge that arose during the implementation of these classifiers was the preparation of the training dataset. While our LSTM-based neural network can easily deal with sequences of variable length, the other classifiers aren't naturally able to process them. If we look at our data then we can see that every gesture consists of several timesteps that each contain 11 feature values. We transformed each data sequence of a gesture into a large vector that has 11 x sequence length entries. We then padded every vector in the dataset with zeros in order for them to have the same length as the longest vector. These modifications allowed us to train the other classifiers with a dataset that consists of data sequences.

\subsection{Live gesture classification tool}

The final goal of our implementation was a tool that allows the live classification of gestures that are performed by a person wearing the J!NS MEME glasses. Creating such a tool opens up the possibilities for applications that use these gestures to execute different types of commands. In order to gain access to the real time data that the J!NS MEME glasses produce during their usage we use a built in function of the J!NS MEME Data Logger. This function writes the data to a specified IP-Adress and Port which allows us to write a tool that accesses the data on this port in real time.
\begin{figure}
\centering
\includegraphics[width=\textwidth]{Live_Classification_IP.png}
\caption{The IP-address and port specification page}
\label{fig:ipandport}
\end{figure}
Figure \ref{fig:ipandport} shows the first page of our Live gesture classification tool. Here the user needs to specify the IP-address and the port that was specified in the J!NS MEME Data Logger. After the user presses the connect button the tool opens a new page where the live classification can start. We use the same procedure that we already used during our data collection process. To start the classification process the user has to press a start button. The tool then automatically records the time of this button press. Then the user performs the gesture. Afterwards he presses the stop button. The time for this button press is also automatically recorded by the tool. Now the tool tries to classify the gesture by first extracting the relevant parts of the data. This is achieved by using the time frame between the start and stop button presses. The extracted data is then fed into our trained neural network. The tool then selects the gesture that is assigned the maximum probability by our neural network and puts out the name of the gesture on the screen.

% Assinged to SA
\section{Results}
\subsection{Baseline}
When deciding on our architecture, we already theorized, that LSTMs should be the most suitable model, due to the time-dependent and time-continuous nature of our data (comp. Chapter \ref{subsec:nnArchAndTraining}). However, to allow for a better understanding and evaluation of the accuracy scores and the performance of our neural networks, we will now first set a baseline that all our results can be compared to. 
\begin{table}
\centering
\caption{Test-Accuracy of popular classification models not chosen in our architecture.}\label{tab:baselineTable}
\begin{tabular}{|l|l|}
    \hline
    Model & Accuracy\\
    \hline
    Random Guess & 9.1\%\\
    Naive Bayes & 14.0\%\\
    K-Nearest-Neighbors & 22.7\%\\
    Decision Tree & 35.5\%\\
    Support Vector Machine & 38.0\%\\
    Random Forest & 61.9\%\\
    \hline
\end{tabular}
\end{table}
% Check whether my formulation is scientific enough, or whether I am being too evaluative
% TODO: insert formula
Since we are micro averaging in a multi-class problem, the F1-score is synonymous to the accuracy value \cite{accuracyEqualsF1Score}.
As we have 11 gestures, just randomly guessing without any classification model yields an accuracy of 9.1\%. The Naive Bayes classifier surprisingly is not much better than guessing with an accuracy of 14.0\%. The simple K-Nearest-Neighbors (KNN) algorithm managed to achieve an accuracy of 22.7\%. Much to our surprise, a single Decision Tree was able to score a 35.5\% accuracy, almost on par with the Support Vector Machine (SVM) that managed 38.0\%. Finally, as expected, the Random Forest classifier far outperforms all other (previously listed) models, boasting an accuracy of 61.9\%. All aforementioned accuracies are taken over the unseen test-set and are therefore test-accuracies.

% TODO: Need a better subsection title
\subsection{Achieved Accuracy}
Compare the aforementioned accuracies (our Baseline, Table \ref{tab:baselineTable}) to the accuracy achieved by our LSTM model:
 The validation accuracy with all 11 gestures included is at 71.9\%. When using the unseen test-set (test-accuracy), our model achieved an accuracy of 73.6\%. 
 
 Upon analysing our results, we noticed, that some gestures are very hard to distinguish for our neural network. The confusion matrices showed, that nose gestures are easily confounded with one another. This is illustrated in Figure \ref{fig:confMat}, showing the confusion matrix of our LSTM with and without the nose gestures. 
\begin{figure}
\centering
\includegraphics[width=\textwidth]{confusionMatrices.png}
\caption{The confusion matrices with nose gestures (right) and without nose gestures (left).}
\label{fig:confMat}
\end{figure}
 Moreover, upon removing some gestures iteratively, we further noticed, that the nose gestures also had a negative influence on the classification of non-nose gestures. Upon removing the nose gestures, our validation and test accuracy shot up to 84.1\% and 83.5\% respectively, underlining our findings. With accuracies of over 71\% over all gestures and over 83\% over all non-nose gestures, this performance lead us to the conclusion that LSTMs are the right choice for our data set and the given problem statement. %gerne letzten satz verbessern, falls noch bessere formulierung einfällt

\subsection{Real-Time-Classifier}
Our trained neural network builds the foundation for the actual use-case: a real-time-classifier (RTC) for gestures executed wearing smart glasses such as the J!NS MEME. For our RTC we tried multiple approaches to record and feed the user data.

We first started with a continuous classification, where all data is directly fed to the RTC. However, as the data was very noisy, it resulted in many false positives and inaccurate classification. We then built a button that would allow one to evaluate the last gesture executed. This was implemented using a predefined fixed time-frame to determine the start of the gesture. But this approach also proved to be too inaccurate due to varying positions of gestures in time-frames. Our final and current version uses a start-and-stop button, analogous to the recording procedure. This not only provides an intuitive and simple operation of the classifier, but by using this approach we were also able to achieve an accuracy of over 70\%.

% Assigned to SA
\section{Discussion}
Our goal was to determine whether or not we could provide new gestures that we then could classify reliably.

This also means that our focus was on training a network that recognises gestures well and implementing a real-time classifier that could take advantage of the trained network. In the process, the interactivity of the real-time classifier took a back seat, which can be observed in our start-and-stop-button based implementation. Naturally, this is not a model that can be readily used with smart glasses in a "hands-free and subtle" way.

Moreover, due to the local Covid-19 situation and time restrictions we could only use a sample of convenience with 10 participants, including ourselves. In other words, our data set was very small. And as we iteratively integrated the participants' data, we saw vast improvements in our accuracy for each participant we added, further underlining the need for more data. We believe that by expanding the data set and increasing the number of participants, an accuracy of over 95\% can definitely be achieved.

Parenthetically, to use the glasses while standing or walking (as has been researched in the Itchy Nose paper \cite{10.1145/3123021.3123060}), the neural network would have to be retrained using new data suited for this purpose.

Overall our results suggest that we can differentiate the proposed gestures very well. Since gestures would most likely be used to subtly and naturally select from options presented on a suitable user interface, 11 gestures (but also the 8 non-nose gestures) should be more than sufficient. We hope to provide a foundation and spark new interest for further research towards gesture classification using smart glasses, that had once started with the Itchy Nose paper \cite{10.1145/3123021.3123060}.

\section{Future Work}
We focused on the collection of new gestures and the training of a neural network for their classification. It would, therefore, be of great interest if future research were to improve on the continuous classification and accuracy of the real-time classifier. Moreover, the accuracy of our neural network model could further be improved upon by expanding the data set and including more participants. Collectively, this would allow for the actual application purpose of subtle gesture control in situations where direct interaction with conventional smart devices is not accepted or well-received.

As all our work has focused on the use of smart glasses in a sedentary environment, further research is needed to enable the use of smart glasses in a standing or walking state. In particular, this would require collecting completely new data sets and retraining the network. Nevertheless, we are optimistic that our presented LSTM architecture provides a good basis for achieving this.

\bibliographystyle{splncs04}
\bibliography{mybibliography}
%
\end{document}
